#!/bin/bash

# Digital Ocean Spaces Backup Script
# CRITICAL: This script uses sync WITHOUT --delete to preserve existing files

# vars declaration
env_name=$(hostname)
timestamp=$(date +"%Y-%m-%d-%H-%M")

db_name="{{ do_backup_db_name }}"
folder_path="{{ do_backup_folder_path }}"
bucket_name="{{ do_spaces_bucket_name }}"
region="{{ do_spaces_region }}"
include_web_folder="{{ do_include_web_folder }}"
aws_path="$(which aws)"

# DO Spaces endpoint
endpoint_url="https://${region}.digitaloceanspaces.com"

# Use separate AWS config for DO Spaces (avoids conflicts with AWS credentials)
export AWS_CONFIG_FILE="/root/.do-spaces/config"
export AWS_SHARED_CREDENTIALS_FILE="/root/.do-spaces/credentials"

# Create local backup folder
mkdir -p /opt/do-backups/temp
cd /opt/do-backups/temp

# Dump the DB. Note .my.cnf needs to be created, otherwise won't be able to dump
mysqldump "$db_name" > "$db_name.sql"

# Compress SQL file with timestamp as filename
tar -zcvf ${timestamp}.tar.gz $db_name.sql

# Remove the SQL file to save space
rm -f ./$db_name.sql

# Upload database backup to DO Spaces: bucket/hostname/databases/YYYY-MM-DD-HH-MM.tar.gz
$aws_path s3 cp /opt/do-backups/temp/${timestamp}.tar.gz \
    s3://$bucket_name/$env_name/databases/ \
    --endpoint-url "$endpoint_url"

# Sync web folder if enabled
# IMPORTANT: NO --delete flag to preserve existing files in the bucket
if [[ "${include_web_folder,,}" == "true" ]]; then
    # Sync to bucket/hostname/web-shared/
    $aws_path s3 sync "$folder_path" \
        s3://$bucket_name/$env_name/web-shared/ \
        --endpoint-url "$endpoint_url"
fi

# Cleanup local backup files
cd /opt/do-backups
rm -rf /opt/do-backups/*
